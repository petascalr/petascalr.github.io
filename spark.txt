Learning Spark - O'Reilly:
Introduction
	Every Spark app has a Driver and multiple Executors that run inside a Cluster.
	A Spark app can be run in 2 modes: Client Mode, Cluster Mode.
	In client mode, the Driver app runs on the client machine, and connects to the Executors inside the Spark Cluster.
	In Cluster mode, the Driver runs along the Executors inside the Cluster.
	- Cluster mode makes more sens in Production.
	- When debugging things or exploring, Client mode is better.

Running a Spark cluster locally:
	./sbin/start-master.sh
	./bin/spark-class org.apache.spark.deploy.worker.Worker  spark://localhost:7077 -c 1 -m 512M
	./bin/spark-submit  --class org.apache.spark.examples.SparkPi   --master spark://localhost:7077  lib/spark-examples-1.2.1-hadoop2.4.0.jar
	
Ch2: Downloading spark and getting started
	- open bin/spark-shell for the Scala Shell
	- Spark is a library that is used from a driver program/function. Driver programs access Spark through a SparkContext object, which represents a connection to a computing cluster
	- In the Spark shell, the SparkContext is already created, and available through the "sc" variable.
	- The driver program typically manages a number of nodes called executors

val lines = sc.textFile("README.md")
val pythonLines = lines.filter(_.contains("Python"))
pythonLines.first

# word count example:
val words = fileLines.flatMap(_.split(" ")) // convert lines into words
val counts = words.map(word => (word, 1)).reduceByKey{case (x, y) => x + y}

Starting a local cluster:

./sbin/start-master.sh
./bin/spark-class org.apache.spark.deploy.worker.Worker  spark://`hostname`:7077 -c 1 -m 512M

bin/run-example SparkPi
./bin/spark-submit --class org.apache.spark.examples.SparkPi --master spark://`hostname`:7077 ./examples/jars/spark-examples_2.12-3.0.1.jar

Ch3: Programming with RDDs:
	- An RDD in Spark is simply an immutable distributed collection of objects. 
	- Transformation & Action for RDDs. Transformations like map, filter, reduce return a new RDD. Action triggers the computation of the pipeline.
	- Transformations return RDDs, whereas actions return some other data type.
	- Although you can define new RDDs any time, Spark computes them only in a lazy fashion—that is, the first time they are used in an action.
	- RDDs are automatically recomputed everytime a action is applied on them. You can also persist()/cache() them, and they won't.
	- The ability to always recompute an RDD is actually why RDDs are called “resilient.” When a machine holding RDD data fails, Spark uses this ability to recompute the missing partitions, transparent to the user.
	- Finally, as you derive new RDDs from each other using transformations, Spark keeps track of the set of dependencies between different RDDs, called the lineage graph.
	- RDDs operations (for binary ops, both RDDs must have the same type):
		.distinct()
		.union(rdd2)
		.intersection(rdd2)
		.subtract(rdd2)
		.carthesian(rdd2) // carthesian product, returns a RDD of pairs.
		
	- RDDs actions
		.reduce((x, y) => x+y)	// returns a single elem of type the same as the elems of RDD.
		.fold((x, y) => x+y) : similar to reduce() but takes a "zero value" that is used as accumulator, eg. 0 for +, 1 for *, etc.
		.collect() gathers all RDD in memory
		.take(n) : keeps only n elements from the RDD.
		
	- RDDs can be persisted in memory/disk:
		- persist(StorageLevel.MEMORY_ONLY)
		- persist(StorageLevel.MEMORY_ONLY_SER)
		- persist(StorageLevel.MEMORY_AND_DISK)
		- persist(StorageLevel.MEMORY_AND_DISK_SERDISK_ONLY)
	- If you attempt to cache too much data to fit in memory, Spark will automatically evict old partitions using a Least Recently Used (LRU) cache policy.
	- When you call collect() on an RDD, all data returned by it must fit in RAM on the current node. (In most practical cases, most RDDs cannot be collect()ed because they are too large).

	lines = sc.textFile("../README.md")
	val wordsOccurences = lines.flatMap(_.split(" ")).map(w => (w, 1)).reduceByKey((x, y) => x + y)
	wordsOccurences.take(10).foreach(println)
	
	
Ch4: Working with Key/Value pairs:
	- KV RDDs are commnly used to perform aggregations, and often we use some ETL (extract, transform, load) to get our data in K-V format. These RDDs are called Pair-RDDs.
	- Pair RDDs are still RDDs of Tuple2 in Java/Scala.
		pairRdd.filter{case (key, val) => value.length < 20 }

	- PairRDDs have a reduceByKey() method that can aggregate data separately per each key.
		- pairRdd.reduceByKey()
		- pairRdd.groupByKey()
		- pairRdd.mapValues()
		
Ch7: Running Spark on a Cluster
	- In distributed mode, Spark uses a master/slave architecture with one central coordinator and many distributed workers. The central coordinator is called the driver.
	- The driver communicates with a potentially large number of distributed workers called executors. The driver runs in its own Java process and each executor is a separate Java process. 
	A driver and its executors are together termed a Spark application.
