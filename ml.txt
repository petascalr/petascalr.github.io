Machine Learning
Purpose of machine learning is to discover recurring patterns in the data. ML tries to make predictions or decisions based on training data.
Sample datasets: https://archive.ics.uci.edu/ml/datasets.php

All learning algorithms require defining a set of features for each item which will be fed into the learning function. In many cases, defining the set of features is the most challenging part of using ML.

An important step is feature extraction and transformation.
Typical steps in a ML pipeline: Featurization -> Feature Vectors -> Training -> Model -> Model Evaluation -> Best Model.

Most learning algorithms have multiple params that can affect results, so real world pipelines will train muyltiple versions of a model and evaluate each one.

* Independent variable: Input into the model (for instance number of hours of study a student put into an exam)
* Dependent variable: The output or outcome resulting from altering these inputs (if the student passed the exam or not). For instance f(x) = 3*X+5=Y. X is the independent var. Y is the dependent var.

* Input variables may be called predictors, or independent variables.
* Outputs may be called responses, or dependent variables.
* Output may be quantitative (continuous) or qualitative (labels). Qualitative variables are also referred to as categorical or discrete or factors.

* A 3rd variable type is ordered cathegorical, such as small/med/large where there is an ordering between the values.

* Linear regression: modelling the relationship between a scalar response (dependent var) and one or more independent variables. If only one input var (indep var) => Simple linear regression. If multiple input vars => Multiple Linear Regression.
* Multivariate Linear Regression => multiple output variables are predicted based on the input(s).

ML can be split in:
	- Supervised Learning (develop predictive model based on both input and output data)
		- Classification (output is a choice between classes/labels: true/false. red/green/blue).
			- The output has discreete values in classification (qualitative, categoric, etc).
			- eg: Email classification: spam/non-spam, based on labeled examples of other items (emails known to be spam or not).
			
		- Regression (output is a number: stock prices, temperature, etc)
			The output is continuous in regression (quantitative).
			- eg: Product recomandation. It is known that person X prefers this list of products. We need to recommend him other products based on his past preferences.

	- Unsupervised Learning (discover an internal representation from input datra only)
		- Clustering: no output; find natural groups and patterns from input data only.
			e.g.: optimal placement of GSM antennas so that all customers receive good service.
	
	- Reinforcement Learning.
	
Method 1: Linear Regression. 
	Given a set of points int the n-th dimensional space, we try to aproximate them with a line. For new points the aproximation will be this line.
	
Method 2: k-NN: k-Nearest Neighbor
	- We find the k-nearest points in the input set (by a metric specified by us, like the Euclidian distance), and we average their responses.
	- If k=1, we always pick the closest point from the input data as the predictor. In this case, for each point coresponds a region in the space that is aproximated to it. This is a Voronoi Tesselation.

Method 3: Decision Trees.
	- If the target variable is discreete, the tree is called a classification tree. If the variable is continuous, the tree is called regression tree.